{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering for archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching data and creating folder structure where data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import json, glob, os, re, requests, time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NYTimes developer console API key\n",
    "nyt_archive_key = os.getenv(\"nyt_archive_key\")\n",
    "print(\"Using NYTimes API key: {}\".format(nyt_archive_key))\n",
    "\n",
    "# nyt_archive_key = os.getenv(\"nyt_key_2\")\n",
    "# print(nyt_archive_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two different APIs to fetch data from added in list\n",
    "apis = ['archive']\n",
    "data_folders_directory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions Definitions:\n",
    "## Function to create folder structure to store data for different APIs in JSON format for question 2\n",
    "def create_directory_for_data(api, verbose=False):\n",
    "    # Relative path for current directory.\n",
    "    current_dir = os.path.dirname('__file__')                                \n",
    "    data_folder = current_dir\n",
    "    return create_subfolders_for_data(data_folder, 'data', 'collection', api, verbose) \n",
    "\n",
    "## Function to create subfolder as per the path specified and API names\n",
    "def create_subfolders_for_data(data_folder, data, question, api, verbose=False):\n",
    "    directory = os.path.join(data_folder, data, question, api)\n",
    "    if not os.path.exists(directory):\n",
    "        if verbose:\n",
    "            print(\"create_subfolders_for_data() - creating directory: {}\".format(directory))\n",
    "            \n",
    "        os.makedirs(directory)\n",
    "\n",
    "    return directory\n",
    "    \n",
    "# Function to write data to JSON file at respective location\n",
    "def write_to_json_file(file_path, json_data, verbose=False):\n",
    "    if verbose:\n",
    "        print(\"write_to_json_file() - deleting file and writing JSON data: {}\".format(file_path))\n",
    "        print(\"write_to_json_file() - JSON data size: {}\".format(len(json_data)))\n",
    "        \n",
    "    with open(file_path, 'w') as json_out:\n",
    "        json.dump(json_data, json_out, indent=2)\n",
    "\n",
    "## Creates a list containing folder paths for both APIs\n",
    "for api in apis:\n",
    "    data_folder_dir = create_directory_for_data(api, verbose=True)\n",
    "    data_folders_directory.append(data_folder_dir)\n",
    "    \n",
    "print(data_folders_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to get the last 6 months with year and month.\n",
    "# Fetching data for 6 previous months \n",
    "def get_year_and_month_range_for_archives(verbose=False):    \n",
    "    year_count = 0\n",
    "    range_of_years = []\n",
    "    mon = 0\n",
    "    \n",
    "    while (year_count < 2):\n",
    "        row = ''\n",
    "        \n",
    "        # Get year in consideration\n",
    "        year_to_consider = str(date.today().year - year_count)\n",
    "        row = str(year_to_consider)\n",
    "        month_count = date.today().month\n",
    "        \n",
    "        # If year is not current year, reset month count to 12 to trace Dec - Jan\n",
    "        if (year_count > 0):\n",
    "            month_count = 12\n",
    "        \n",
    "        # If month is before January, change the year\n",
    "        while (month_count > 0):\n",
    "            if (year_count == 1) and (mon > 5):         # Limiting the search for 6 months that's why 0-5\n",
    "                break\n",
    "            row = str(year_to_consider) + ',' + str(month_count)\n",
    "            range_of_years.append(row)\n",
    "            mon += 1\n",
    "            month_count -= 1\n",
    "        year_count += 1\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"get_year_and_month_range_for_archives() - returning: {}\".format(range_of_years))\n",
    "        \n",
    "    return range_of_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to fetch response from API provided\n",
    "## As both APIs have different calling parameters, using it to differentiate between the calls\n",
    "def fetch_response_from_api(page_count, api, year, month, verbose=False):\n",
    "    response = None\n",
    "    if api == 'archive':\n",
    "        # URL to hit\n",
    "        url = 'https://api.nytimes.com/svc/' + api + '/v1/' + year + '/' + month + '.json'\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"fetch_response_from_api() - URL is {}\".format(url))\n",
    "            \n",
    "        # Parameters to pass\n",
    "        payload = {'api-key': nyt_archive_key}\n",
    "        response = requests.get(url, params=payload)\n",
    "    \n",
    "    # Add time delay between 2 API calls to fetch response without interruption\n",
    "    time.sleep(1)\n",
    "\n",
    "    return response \n",
    "\n",
    "# Returns response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to extract only articles from the response file\n",
    "## Also checking if the article is already present in file or not. \n",
    "## If article is present already, don't append it to list of articles.\n",
    "def process_response_from_service(response, api, file_path, page_count, verbose=False):\n",
    "    if response.status_code == 200:\n",
    "        res = response.json()\n",
    "                \n",
    "        if verbose:\n",
    "            print(\"process_response_from_service() - res['response']['docs'] size: {}\".format(len(res['response']['docs'])))\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            if verbose:\n",
    "                print(\"process_response_from_service() - opening file: {}\".format(file_path))\n",
    "                \n",
    "            with open(file_path) as fil:\n",
    "                # Get already present file and its content as we will use this multiple times to gather data\n",
    "                json_data = json.load(fil)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"process_response_from_service() - current data size: {}\".format(len(json_data)))\n",
    "                \n",
    "                # Remove duplicates.\n",
    "                unseen_articles = [artic for artic in res['response']['docs'] if artic['_id'] not in [articles['_id'] for articles in json_data]]\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"process_response_from_service() - unseen articles size: {}\".format(len(unseen_articles)))\n",
    "\n",
    "                json_data.extend(unseen_articles)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"process_response_from_service() - file {} doesn't exist\".format(file_path))\n",
    "\n",
    "            # If the file is not present, don't check for duplicates. Just write the articles into variable\n",
    "            json_data = res['response']['docs']\n",
    "        \n",
    "        # Write output to JSON format.\n",
    "        write_to_json_file(file_path, json_data, verbose=True)\n",
    "    else:\n",
    "        # Show error messages in case an API fails\n",
    "        print(\"process_response_from_service() - request failed for '{}' with status code: {}\"\n",
    "              .format(api, response.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_folders_directory)\n",
    "for data_folder_dir in data_folders_directory:\n",
    "    page_count = 0\n",
    "    \n",
    "    # Get the name of API from last component of the path\n",
    "    api = os.path.split(data_folder_dir)[1]\n",
    "    print('API to hit -> {}'.format(api))\n",
    "    \n",
    "    # Create the name of JSON file with folder path\n",
    "    file_name = api + '_response_pages.json'\n",
    "    file_path = os.path.join(data_folder_dir, file_name)\n",
    "\n",
    "    print(\"file_path is: {}\".format(file_path))\n",
    "\n",
    "    if api == 'archive':\n",
    "        # Get year and month range for past 6 months\n",
    "        year_range_for_archive = get_year_and_month_range_for_archives(verbose=True)\n",
    "        for time_to_consider in year_range_for_archive:\n",
    "            # year and month is used but not page count\n",
    "            year = time_to_consider[:4]\n",
    "            month = time_to_consider[5:]\n",
    "                \n",
    "            # Fetch response for each year and month for past 6 months\n",
    "            response = fetch_response_from_api(page_count, api, year, month, verbose=True)\n",
    "                \n",
    "            # save the articles only from response into JSON file \n",
    "            process_response_from_service(response, api, file_path, page_count, verbose=True)\n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            page_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to check how much data is present in each file.\n",
    "#### No need to run. It will just print the number of articles in each response\n",
    "for data_folder_dir in data_folders_directory:\n",
    "    api = data_folder_dir[15:]\n",
    "    print('API - ', api)\n",
    "    \n",
    "    file_name = api + '_response_pages.json'\n",
    "    file_path = os.path.join(data_folder_dir, file_name)\n",
    "\n",
    "    with open(file_path) as file_to_read:\n",
    "        present = json.load(file_to_read)\n",
    "        print('Data Count - ', len(present))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data_folders_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
